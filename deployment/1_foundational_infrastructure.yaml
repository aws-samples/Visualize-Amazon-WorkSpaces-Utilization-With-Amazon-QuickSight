AWSTemplateFormatVersion: '2010-09-09'
Description: Visualize Amazon WorkSpaces Utilization with Amazon QuickSight - 1 Foundational Infrastructure

Mappings:
  Options:
    global:      
      WorkSpacesManifestJsonFileName: workspaces.json
      CostOptimizerDataFileName: aggregated_dry-run_daily.csv

Parameters:
  CostOptimizerBucketName:
    Type: String
    Description: The name of the CostOptimizer bucket created by the Cost Optimizer for Amazon WorkSpaces.

Resources:
  WorkSpacesDataBucket:
    Type: AWS::S3::Bucket
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W35
            reason: "By default, S3 does not collect server access logs."
      #checkov:skip=CKV_AWS_18: By default, S3 does not collect server access logs.
      #checkov:skip=CKV_AWS_21: The S3 bucket does not need versioning enabled.
    Properties:
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true

  S3BucketPolicy:
    Type: AWS::S3::BucketPolicy
    Properties:
      Bucket:
        Ref: WorkSpacesDataBucket
      PolicyDocument:
        Statement:
          - Action: s3:*
            Condition:
              Bool:
                aws:SecureTransport: "false"
            Effect: Deny
            Principal:
              AWS: "*"
            Resource:
              - Fn::GetAtt:
                  - WorkSpacesDataBucket
                  - Arn
              - Fn::Join:
                  - ""
                  - - Fn::GetAtt:
                        - WorkSpacesDataBucket
                        - Arn
                    - /*
        Version: "2012-10-17"

  LambdaExecutionWorkSpacesRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: 
                - lambda.amazonaws.com
            Action: 'sts:AssumeRole'
      Policies:
        - PolicyName: LambdaExecutionWorkSpacesPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - logs:CreateLogGroup
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                Resource: 
                  - !Sub 'arn:${AWS::Partition}:logs:${AWS::Region}:${AWS::AccountId}:log-group:/aws/lambda/*'
              - Effect: Allow
                Action:
                  - s3:GetObject
                  - s3:GetObjectVersion
                  - s3:GetBucketLocation
                  - s3:PutObject
                  - s3:ListBucket
                  - s3:ListBucketVersions
                Resource:
                  - !Sub 'arn:${AWS::Partition}:s3:::${WorkSpacesDataBucket}'
                  - !Sub 'arn:${AWS::Partition}:s3:::${WorkSpacesDataBucket}/*'
              - Effect: Allow
                Action:
                  - s3:GetObject
                  - s3:ListBucket
                  - s3:GetBucketNotification
                  - s3:PutBucketNotification
                Resource:
                  - !Sub 'arn:${AWS::Partition}:s3:::${CostOptimizerBucketName}'
                  - !Sub 'arn:${AWS::Partition}:s3:::${CostOptimizerBucketName}/*'

  CopyCostOptimizerData:
    Type: AWS::Lambda::Function
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W89
            reason: "The Lambda function does not need access to resources in a VPC."
      #checkov:skip=CKV_AWS_116: The Lambda function does not need a Dead Letter Queue (DLQ).
      #checkov:skip=CKV_AWS_117: The Lambda function does not need access to resources in a VPC.
      #checkov:skip=CKV_AWS_173: The Lambda function environment variables hold nonsensitive S3 bucket and file names.
    Properties:
      Description: Workspaces - Copies latest WorkSpaces utilization file to WorkSpacesDataBucket for QuickSight Datasource
      Handler: index.lambda_handler
      Role: !GetAtt LambdaExecutionWorkSpacesRole.Arn
      Code:
        ZipFile: |
          import boto3
          import logging
          import json
          import os

          from datetime import datetime, timedelta

          logger = logging.getLogger()
          logger.setLevel(logging.INFO)

          s3 = boto3.client('s3')

          def lambda_handler(event, context):
            logger.info("Received event: %s", event)

            # Calculate the date of the previous day
            previous_day = datetime.today() - timedelta(days=1)
            subfolder_date = previous_day.strftime('%Y/%m/%d')  # Format: YYYY/MM/DD
              
            source_bucket = os.environ['COST_OPTIMIZER_BUCKET_NAME']
            source_key = f"{subfolder_date}/{os.environ['COST_OPTIMIZER_DATA_FILE_NAME']}"
            destination_bucket = os.environ['WORKSPACES_DATA_BUCKET_NAME']
            destination_key = os.environ['COST_OPTIMIZER_DATA_FILE_NAME']
              
            logger.info("Source Bucket: %s", source_bucket)
            logger.info("Source  Key: %s", source_key)
            logger.info("Destination Bucket: %s", destination_bucket)
            logger.info("Destination  Key: %s", destination_key)
              
            try:
              # Copy the CSV file
              s3.copy_object(
                Bucket=destination_bucket,
                Key=destination_key,
                CopySource={'Bucket': source_bucket, 'Key': source_key}
              )
              
              logger.info("S3 object copied successfully")
              
              return {
                "statusCode": 200,
                "body": json.dumps({
                  "message": "S3 object copied successfully",
                  "bucket": destination_bucket,
                  "key": destination_key
                })
              }

            except Exception as e:
              logger.error("Error copying S3 object: %s", e)
              
              return {
                "statusCode": 500,
                "body": json.dumps({"error": str(e)})
              }
      Runtime: python3.12
      Timeout: 15
      ReservedConcurrentExecutions: 1
      Environment:
        Variables:
          COST_OPTIMIZER_BUCKET_NAME: !Ref CostOptimizerBucketName
          COST_OPTIMIZER_DATA_FILE_NAME: !FindInMap [Options, global, CostOptimizerDataFileName]
          WORKSPACES_DATA_BUCKET_NAME: !Ref WorkSpacesDataBucket    

  LambdaInvokePermission:
    Type: AWS::Lambda::Permission
    Properties:
      Action: lambda:InvokeFunction
      FunctionName: !GetAtt CopyCostOptimizerData.Arn
      Principal: s3.amazonaws.com
      SourceArn: !Sub "arn:${AWS::Partition}:s3:::${CostOptimizerBucketName}"

  cfnGenerateWorkSpacesManifestContent:
    Type: AWS::Lambda::Function
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W89
            reason: "The Lambda function does not need access to resources in a VPC."
      #checkov:skip=CKV_AWS_116: The Lambda function does not need a Dead Letter Queue (DLQ).
      #checkov:skip=CKV_AWS_117: The Lambda function does not need access to resources in a VPC.
    Properties:
      Description: Generates the manifest content for the QuickSight data source
      Handler: index.lambda_handler
      Role: !GetAtt LambdaExecutionWorkSpacesRole.Arn
      Code:
        ZipFile: |
          import cfnresponse
          import json
          import logging

          logger = logging.getLogger()
          logger.setLevel(logging.INFO)

          def lambda_handler(event, context):
            logger.info("Received event: %s", json.dumps(event))

            try:
              work_spaces_data_bucket = event['ResourceProperties']['WorkSpacesDataBucket']
              cost_optimizer_data_file_name = event['ResourceProperties']['CostOptimizerDataFileName']

              logger.info("WorkSpacesDataBucket: %s", work_spaces_data_bucket)
              logger.info("CostOptimizerDataFileName: %s", cost_optimizer_data_file_name)

              manifest_content = {
                "fileLocations": [
                  {
                    "URIs": [
                      f"s3://{work_spaces_data_bucket}/{cost_optimizer_data_file_name}"
                    ]
                  }
                ],
                "globalUploadSettings": {
                  "format": "CSV",
                  "delimiter": ",",
                  "textqualifier": "'",
                  "containsHeader": "true"
                }
              }
      
              logger.info("Generated manifest content: %s", json.dumps(manifest_content))

              cfnresponse.send(event, context, cfnresponse.SUCCESS, {"Content": json.dumps(manifest_content)})
            except Exception as e:
              logger.error("Error sending response to CloudFormation: %s", str(e))
              cfnresponse.send(event, context, cfnresponse.FAILED, {"Error": str(e)})
      Runtime: python3.12
      Timeout: 15
      ReservedConcurrentExecutions: 1

  InvokeCfnGenerateWorkSpacesManifestContent:
    Type: Custom::InvokeLambda
    Properties:
      ServiceToken: !GetAtt cfnGenerateWorkSpacesManifestContent.Arn
      WorkSpacesDataBucket: !Ref WorkSpacesDataBucket
      CostOptimizerDataFileName: !FindInMap [Options, global, CostOptimizerDataFileName]

  cfnCreateWorkSpacesManifest:
    Type: AWS::Lambda::Function
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W89
            reason: "The Lambda function does not need access to resources in a VPC."
      #checkov:skip=CKV_AWS_116: The Lambda function does not need a Dead Letter Queue (DLQ).
      #checkov:skip=CKV_AWS_117: The Lambda function does not need access to resources in a VPC.
    Properties:
      Description: Workspaces - Creates S3 JSON manifest file for QuickSight Datasource
      Handler: index.lambda_handler
      Role: !GetAtt LambdaExecutionWorkSpacesRole.Arn
      Code:
        ZipFile: |
          import boto3
          import cfnresponse
          import json
          import logging
          
          logger = logging.getLogger()
          logger.setLevel(logging.INFO)

          s3 = boto3.client('s3')

          def lambda_handler(event, context):
            logger.info("Received event: %s", event)    

            if event['RequestType'] == 'Delete':
              logger.info("Delete event detected.")
              cfnresponse.send(event, context, cfnresponse.SUCCESS, {"Message": "Delete event detected."})
              return

            destination_bucket = event['ResourceProperties']['WorkSpacesDataBucket']
              
            key = event['ResourceProperties']['WorkSpacesManifestJsonFileName']
              
            content = json.loads(event['ResourceProperties']['Content'])
            
            logger.info("WorkSpacesDataBucket: %s", destination_bucket)
            logger.info("Key: %s", key)
            logger.info("Content: %s", content)

            try:
              logger.info("Creating WorkSpaces manifest.")
              
              response = s3.put_object(Bucket=destination_bucket, Key=key, Body=json.dumps(content, indent=2), ContentType='application/json')
              
              logger.info("Successfully created WorkSpaces manifest: %s", response)
                  
              cfnresponse.send(event, context, cfnresponse.SUCCESS, {"Message": "Successfully created WorkSpaces manifest", "Bucket": destination_bucket, "Key": key}, "CustomResourcePhysicalID")

            except Exception as e:
              logger.error("Error creating WorkSpaces manifest: %s", e)
              cfnresponse.send(event, context, cfnresponse.FAILED, {"Message": f"Error creating S3 object: {str(e)}, Event: {json.dumps(event)}"}, "CustomResourcePhysicalID")
      Runtime: python3.12
      Timeout: 15
      ReservedConcurrentExecutions: 1

  InvokeCfnCreateWorkSpacesManifest:
    Type: Custom::S3Object
    Properties:
      ServiceToken: !GetAtt cfnCreateWorkSpacesManifest.Arn
      WorkSpacesDataBucket: !Ref WorkSpacesDataBucket
      WorkSpacesManifestJsonFileName: !FindInMap [Options, global, WorkSpacesManifestJsonFileName]
      Content: !GetAtt InvokeCfnGenerateWorkSpacesManifestContent.Content
  
  cfnCopyInitialCostOptimizerData:
    Type: AWS::Lambda::Function
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W89
            reason: "The Lambda function does not need access to resources in a VPC."
      #checkov:skip=CKV_AWS_116: The Lambda function does not need a Dead Letter Queue (DLQ).
      #checkov:skip=CKV_AWS_117: The Lambda function does not need access to resources in a VPC.
    Properties:
      Description: Workspaces - Copies latest WorkSpaces utilization file to WorkSpacesDataBucket for QuickSight Datasource
      Handler: index.lambda_handler
      Role: !GetAtt LambdaExecutionWorkSpacesRole.Arn
      Code:
        ZipFile: |
          import boto3
          import cfnresponse
          import json
          import logging

          from datetime import datetime, timedelta

          logger = logging.getLogger()
          logger.setLevel(logging.INFO)

          s3 = boto3.client('s3')

          def lambda_handler(event, context):

            logger.info("Received event: %s", event)

            if event['RequestType'] == 'Delete':
              logger.info("Delete event detected.")
              cfnresponse.send(event, context, cfnresponse.SUCCESS, {"Message": "Delete event detected."})
              return

            destination_bucket = event['ResourceProperties']['DestinationBucket']           
            logger.info("Destination Bucket: %s", destination_bucket)

            destination_key = event['ResourceProperties']['KeyName']            
            logger.info("Destination Key: %s", destination_key)

            source_bucket = event['ResourceProperties']['SourceBucket']
            logger.info("Source Bucket: %s", source_bucket)

            root_objects = s3.list_objects_v2(Bucket=source_bucket, Delimiter='/')
            years = [obj.get('Prefix').rstrip('/') for obj in root_objects.get('CommonPrefixes', []) if obj.get('Prefix').rstrip('/')]
            
            latest_year = max(years, key=lambda x: int(x))
            logger.info("Latest Year: %s", latest_year)
            
            year_objects = s3.list_objects_v2(Bucket=source_bucket, Prefix=f"{latest_year}/", Delimiter='/')
            months = [obj.get('Prefix').rstrip('/').split('/')[1] for obj in year_objects.get('CommonPrefixes', []) if obj.get('Prefix').rstrip('/').split('/')[1]]
            
            latest_month = max(months, key=lambda x: int(x))
            logger.info("Latest Month: %s", latest_month)

            month_objects = s3.list_objects_v2(Bucket=source_bucket, Prefix=f"{latest_year}/{latest_month}/", Delimiter='/')
            days = [obj.get('Prefix').rstrip('/').split('/')[2] for obj in month_objects.get('CommonPrefixes', []) if obj.get('Prefix').rstrip('/').split('/')[2]]
            latest_day = max(days, key=lambda x: int(x))
            logger.info("Latest Day: %s", latest_day)
              
            source_key = f"{latest_year}/{latest_month}/{latest_day}/{event['ResourceProperties']['KeyName']}"
            logger.info("Source Key: %s", source_key)
              
            try:
              logger.info("Copying CostOptimizer data.")
              
              s3.copy_object(Bucket=destination_bucket, Key=destination_key, CopySource={'Bucket': source_bucket, 'Key': source_key})

              logger.info("Successfully copied CostOptimizer data.")
              
              cfnresponse.send(event, context, cfnresponse.SUCCESS, {"Message": "Successfully copied CostOptimizer data.", "Bucket": destination_bucket, "Key": destination_key})

            except Exception as e:
              logger.error("Error copying CostOptimizer data: %s", e)

              cfnresponse.send(event, context, cfnresponse.FAILED, {"Message": f"Error copying CostOptimizer data: {str(e)}, Event: {json.dumps(event)}"})
      Runtime: python3.12
      Timeout: 15
      ReservedConcurrentExecutions: 1

  InvokeCfnCopyInitialCostOptimizerData:
    Type: Custom::InvokeLambda
    Properties:
      ServiceToken: !GetAtt cfnCopyInitialCostOptimizerData.Arn
      SourceBucket: !Ref CostOptimizerBucketName
      DestinationBucket: !Ref WorkSpacesDataBucket
      KeyName: !FindInMap [Options, global, CostOptimizerDataFileName]

  cfnCreateCostOptimizerBucketS3EventNotification:
    Type: AWS::Lambda::Function
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W89
            reason: "The Lambda function does not need access to resources in a VPC."
      #checkov:skip=CKV_AWS_116: The Lambda function does not need a Dead Letter Queue (DLQ).
      #checkov:skip=CKV_AWS_117: The Lambda function does not need access to resources in a VPC.
    Properties:
      Handler: index.handler
      Role: !GetAtt LambdaExecutionWorkSpacesRole.Arn
      Code:
        ZipFile: |
          import boto3
          import json
          import logging
          import cfnresponse

          logger = logging.getLogger()
          logger.setLevel(logging.INFO)

          s3 = boto3.client('s3')

          def handler(event, context):
            logger.info("Received event: %s", json.dumps(event))

            bucket = event['ResourceProperties']['BucketName']
            source_key = event['ResourceProperties']['SourceKey']
            notification_configuration = event['ResourceProperties']['NotificationConfiguration']
            
            logger.info("Bucket: %s", bucket)
            logger.info("Source Key: %s", source_key)
            logger.info("Notification Configuration: %s", json.dumps(notification_configuration))
            
            if event['RequestType'] == 'Delete':
              try:
                logger.info("Deleting S3 bucket notification configuration for bucket: %s", bucket)
                
                existing_config = s3.get_bucket_notification_configuration(Bucket=bucket)
                
                logger.info("Existing notification configuration: %s", json.dumps(existing_config))
                
                existing_config['LambdaFunctionConfigurations'] = [
                  config for config in existing_config.get('LambdaFunctionConfigurations', [])
                  if config['LambdaFunctionArn'] != notification_configuration['LambdaFunctionConfigurations'][0]['LambdaFunctionArn']
                ]

                if 'ResponseMetadata' in existing_config:
                  del existing_config['ResponseMetadata']
                
                logger.info("Updated notification configuration: %s", json.dumps(existing_config))
                
                s3.put_bucket_notification_configuration(Bucket=bucket, NotificationConfiguration=existing_config)
                
                logger.info("S3 bucket notification configuration deleted successfully")
                
                cfnresponse.send(event, context, cfnresponse.SUCCESS, {"Message": f"S3 bucket notification configuration deleted successfully: {json.dumps(event)}"})
              
              except Exception as e:
                logger.error("Error deleting S3 bucket notification: %s", e)
                cfnresponse.send(event, context, cfnresponse.FAILED, {"Message": f"Error deleting S3 bucket notification: {str(e)}, Event: {json.dumps(event)}"})
              
            else:
              try:
                logger.info("Setting up S3 bucket notification configuration for bucket: %s", bucket)
                
                logger.info("Notification configuration: %s", json.dumps(notification_configuration))
                
                s3.put_bucket_notification_configuration(Bucket=bucket, NotificationConfiguration=notification_configuration)

                logger.info("S3 bucket notification configuration updated successfully")
                
                cfnresponse.send(event, context, cfnresponse.SUCCESS, {"Message": f"S3 bucket notification configuration updated successfully: {json.dumps(event)}"})
              
              except Exception as e:
                logger.error("Error setting up S3 bucket notification: %s", e)
                
                cfnresponse.send(event, context, cfnresponse.FAILED, {"Message": f"Error setting up S3 bucket notification: {str(e)}, Event: {json.dumps(event)}"})
      Runtime: python3.12
      Timeout: 15
      ReservedConcurrentExecutions: 1

  InvokeCfnCreateCostOptimizerBucketS3EventNotification:
    Type: Custom::S3Notification
    Properties:
      ServiceToken: !GetAtt cfnCreateCostOptimizerBucketS3EventNotification.Arn
      BucketName: !Ref CostOptimizerBucketName
      SourceKey: !FindInMap [Options, global, CostOptimizerDataFileName]
      NotificationConfiguration:
        LambdaFunctionConfigurations:
          - LambdaFunctionArn: !GetAtt CopyCostOptimizerData.Arn
            Events:
              - s3:ObjectCreated:*
            Filter:
              Key:
                FilterRules:
                  - Name: suffix
                    Value: !FindInMap [Options, global, CostOptimizerDataFileName]

Outputs:
  WorkSpacesDataBucketName:
    Description: The name of the WorkSpacesDataBucket
    Value: !Ref WorkSpacesDataBucket